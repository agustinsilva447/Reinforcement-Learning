{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzdAesJVad6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "b64532f9-1960-493a-c4b9-42b8f4c59d21"
      },
      "source": [
        "! pip install tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77ho5sr80keL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYblsDye0nz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "PERCENTILE = 70"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNJEZL1W0rCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILu0qQgw0tUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_batches(env, net, batch_size):\n",
        "    batch = []\n",
        "    episode_reward = 0.0\n",
        "    episode_steps = []\n",
        "    obs = env.reset()\n",
        "    sm = nn.Softmax(dim=1)\n",
        "    while True:\n",
        "        obs_v = torch.FloatTensor([obs])\n",
        "        act_probs_v = sm(net(obs_v))\n",
        "        act_probs = act_probs_v.data.numpy()[0]\n",
        "        action = np.random.choice(len(act_probs), p=act_probs)\n",
        "        next_obs, reward, is_done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        step = EpisodeStep(observation=obs, action=action)\n",
        "        episode_steps.append(step)\n",
        "        if is_done:\n",
        "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
        "            batch.append(e)\n",
        "            episode_reward = 0.0\n",
        "            episode_steps = []\n",
        "            next_obs = env.reset()\n",
        "            if len(batch) == batch_size:\n",
        "                yield batch\n",
        "                batch = []\n",
        "        obs = next_obs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17nJNidK0wLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_batch(batch, percentile):\n",
        "    rewards = list(map(lambda s: s.reward, batch))\n",
        "    reward_bound = np.percentile(rewards, percentile)\n",
        "    reward_mean = float(np.mean(rewards))\n",
        "\n",
        "    train_obs = []\n",
        "    train_act = []\n",
        "    for reward, steps in batch:\n",
        "        if reward < reward_bound:\n",
        "            continue\n",
        "        train_obs.extend(map(lambda step: step.observation, steps))\n",
        "        train_act.extend(map(lambda step: step.action, steps))\n",
        "\n",
        "    train_obs_v = torch.FloatTensor(train_obs)\n",
        "    train_act_v = torch.LongTensor(train_act)\n",
        "    return train_obs_v, train_act_v, reward_bound, reward_mean"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQeTq5H-0yV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "# env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
        "obs_size = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
        "writer = SummaryWriter(comment=\"-cartpole\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSs9ypwB01sF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "494bd58c-be7d-4a96-ed60-6440a0885135"
      },
      "source": [
        "for iter_no, batch in enumerate(iterate_batches(\n",
        "        env, net, BATCH_SIZE)):\n",
        "    obs_v, acts_v, reward_b, reward_m = \\\n",
        "        filter_batch(batch, PERCENTILE)\n",
        "    optimizer.zero_grad()\n",
        "    action_scores_v = net(obs_v)\n",
        "    loss_v = objective(action_scores_v, acts_v)\n",
        "    loss_v.backward()\n",
        "    optimizer.step()\n",
        "    print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
        "        iter_no, loss_v.item(), reward_m, reward_b))\n",
        "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
        "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
        "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
        "    if reward_m > 199:\n",
        "        print(\"Solved!\")\n",
        "        break\n",
        "writer.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: loss=0.692, reward_mean=19.7, rw_bound=22.0\n",
            "1: loss=0.690, reward_mean=21.6, rw_bound=24.0\n",
            "2: loss=0.679, reward_mean=19.4, rw_bound=22.0\n",
            "3: loss=0.674, reward_mean=32.0, rw_bound=40.5\n",
            "4: loss=0.657, reward_mean=30.5, rw_bound=37.0\n",
            "5: loss=0.642, reward_mean=32.9, rw_bound=38.5\n",
            "6: loss=0.629, reward_mean=40.4, rw_bound=44.5\n",
            "7: loss=0.626, reward_mean=44.1, rw_bound=44.5\n",
            "8: loss=0.610, reward_mean=54.1, rw_bound=67.5\n",
            "9: loss=0.591, reward_mean=57.1, rw_bound=67.0\n",
            "10: loss=0.597, reward_mean=50.6, rw_bound=59.0\n",
            "11: loss=0.597, reward_mean=59.4, rw_bound=62.0\n",
            "12: loss=0.591, reward_mean=60.4, rw_bound=72.5\n",
            "13: loss=0.586, reward_mean=87.1, rw_bound=104.5\n",
            "14: loss=0.557, reward_mean=61.9, rw_bound=70.5\n",
            "15: loss=0.560, reward_mean=61.4, rw_bound=72.5\n",
            "16: loss=0.564, reward_mean=80.7, rw_bound=92.5\n",
            "17: loss=0.544, reward_mean=82.4, rw_bound=87.0\n",
            "18: loss=0.542, reward_mean=72.6, rw_bound=75.0\n",
            "19: loss=0.546, reward_mean=75.2, rw_bound=74.0\n",
            "20: loss=0.549, reward_mean=94.4, rw_bound=106.0\n",
            "21: loss=0.531, reward_mean=64.1, rw_bound=69.5\n",
            "22: loss=0.544, reward_mean=93.8, rw_bound=125.5\n",
            "23: loss=0.529, reward_mean=83.6, rw_bound=98.0\n",
            "24: loss=0.520, reward_mean=94.4, rw_bound=94.5\n",
            "25: loss=0.551, reward_mean=110.9, rw_bound=126.5\n",
            "26: loss=0.521, reward_mean=97.3, rw_bound=117.5\n",
            "27: loss=0.524, reward_mean=113.5, rw_bound=131.5\n",
            "28: loss=0.516, reward_mean=123.2, rw_bound=139.0\n",
            "29: loss=0.521, reward_mean=126.1, rw_bound=164.0\n",
            "30: loss=0.515, reward_mean=116.9, rw_bound=136.0\n",
            "31: loss=0.532, reward_mean=113.1, rw_bound=130.5\n",
            "32: loss=0.521, reward_mean=133.4, rw_bound=161.0\n",
            "33: loss=0.506, reward_mean=130.2, rw_bound=151.5\n",
            "34: loss=0.526, reward_mean=138.8, rw_bound=167.0\n",
            "35: loss=0.503, reward_mean=151.4, rw_bound=179.0\n",
            "36: loss=0.519, reward_mean=174.2, rw_bound=200.0\n",
            "37: loss=0.513, reward_mean=190.6, rw_bound=200.0\n",
            "38: loss=0.513, reward_mean=200.0, rw_bound=200.0\n",
            "Solved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUPuhMTa06FX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}